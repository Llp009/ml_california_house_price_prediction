---
title: "mlph_proj"
author: "zhanpeng Li"
date: "2023-04-23"
output:
  pdf_document: 
    toc: yes
    latex_engine: "xelatex"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. library
```{r,warning=FALSE}
library(r02pro) #INSTALL IF NECESSARY
library(tidyverse) #INSTALL IF NECESSARY
library(MASS)
library(tree)
library(dplyr)
library(caret)
library(randomForest)
library(pROC)
```

# 2. import data
```{r,warning = FALSE}
California_Houses <- read.csv("California_Houses.csv")
set.seed(123)
California_Houses_reg <- California_Houses %>% dplyr::select(-Latitude, -Longitude) %>% na.omit()

train_idx <- sample(nrow(California_Houses_reg), nrow(California_Houses_reg) * 0.1)
train_df <- California_Houses_reg[train_idx, ]
test_df <- California_Houses_reg[-train_idx, ]
```

# 3. Scale all features into the interval $[0, 1]$.

```{r}
std_fit <- preProcess(train_df, method = "scale")
train_std <- predict(std_fit, newdata = train_df)
std_fit_test <- preProcess(test_df, method = "scale")
test_std <- predict(std_fit_test, newdata = test_df)
```

# 4. Decision Tree

```{r}
#decision tree with CV pruning.

my_control <- tree.control(nrow(train_std),mincut=50, minsize = 200, mindev = 0)
fit <- tree(Median_House_Value ~ . ,control = my_control,data = train_std)
set.seed(0)
cv.fit <- cv.tree(fit)
cv.fit_df <- data.frame(size = cv.fit$size, deviance = cv.fit$dev)
least_dev <- min(cv.fit$dev)
best_size <- min(cv.fit$size[cv.fit$dev == least_dev])

ggplot(cv.fit_df, mapping = aes(x = size, y = deviance)) +
geom_point(size = 1) +
geom_line() +
geom_vline(xintercept = best_size, col = "red")


fit.tree<-prune.tree(fit, best = best_size)
pred_fit <- predict(fit.tree, newdata = train_std)
train_error<- mean((pred_fit - train_std$Median_House_Value)^2)
train_error

pred_fit_te<- predict(fit.tree,newdata = test_std)
test_error<- mean((pred_fit_te - test_std$Median_House_Value)^2)
test_error

plot(fit.tree)
text(fit.tree,  all = TRUE, cex = 0.5)

#Evaluation
summary(fit.tree)

```

# 5.random forest
```{r}
set.seed(1)
rf <- randomForest(Median_House_Value ~ .,data = train_std,importance = TRUE)
pred_rf<- predict(rf, newdata =train_std)
train_error<- mean((pred_rf - train_std$Median_House_Value)^2)
train_error

pred_rf_te<- predict(rf, newdata =test_std)
test_error<- mean((pred_rf_te - test_std$Median_House_Value)^2)
test_error

#Evaluation
summary(rf)
plot(rf, main = "Variable Importance Plot")
```

# 6. visualization
```{r}
index<- seq_len(nrow(train_std))
cp1<- rbind(data.frame(Index = index, Price = pred_rf, type = "randomForest_train"),
            data.frame(Index = index, Price = train_std$Median_House_Value, type = "train_std"))
cp2<- rbind(data.frame(Index = index, Price = pred_fit, type = "decision_tree_train"),
      data.frame(Index = index, Price = train_std$Median_House_Value, type = "train_std"))
ggplot(cp1, mapping = aes(x = Index, y = Price, color = type)) +
geom_point()
ggplot(cp2, mapping = aes(x = Index, y = Price, color = type)) +
geom_point()
```

